# Creating an Azure AI Search Index
For the AI agent developed using Azure AI Agent Service, we will create an index in Azure AI Search, store searchable documents within it, and make it ready for querying. There are two main methods for storing searchable documents in an index: the Pull model and the Push model.

![Two methods of data ingestion into an AI Search index](images/99.others/ai-search-pull-and-push.png)

The Pull model uses Azure AI Search’s features—such as data sources, indexers, and skillsets—to periodically access the data store, collect and process data, and then store it in the index. Because these processes are implemented using only JSON definitions (low-code), data ingestion is achieved with minimal effort.

On the other hand, the Push model stores searchable documents into the index via the [AI Search REST API](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/?view=rest-searchservice-2024-07-01&tabs=HTTP). Since this requires you to implement custom programming, its advantage is the flexibility in creating searchable documents.

In this guide, we will prepare an index using the Pull model. To set up the index, we will create the following components either through JSON definitions or via GUI operations:
- Index
- Data source
- Skillset
- Indexer

## Preparing the Index
As before, click on the AI Search account you will use from the list of Azure resources on the resource group page to display its details.

![Displaying the AI Search account page](images/3.prepare-index/1.png)

### Creating the Index
On the AI Search account page, click on ```[Indexes]``` in the search management section of the side menu. This displays the list of indexes created for the account. Then, click on ```[+ Add Index]``` in the top menu, followed by choosing ```[Add Index (JSON)]```.

![Creating an AI Search index](images/3.prepare-index/2.png)

A screen for adding an index via a JSON specification appears. Essentially, you should input the JSON definition from [index.json](../ai-search/index.json#L117), but replace the ```{OPENAI_SERVICE_NAME}``` at line 117 with the name of the AI Service account that was granted the role for AI Search.

```json
"vectorizers": [
    {
        "name": "vectorizer",
        "kind": "azureOpenAI",
        "azureOpenAIParameters": {
            "resourceUri": "https://{OPENAI_SERVICE_NAME}.openai.azure.com",
            "deploymentId": "text-embedding-3-large",
            "modelName": "text-embedding-3-large"
        }
    }
],
```

![Creating an AI Search index](images/3.prepare-index/3'.png)

Replace the originally entered JSON with the corrected JSON definition and click the ```[Save]``` button.

![Creating an AI Search index](images/3.prepare-index/3.png)

### Creating the Data Source
Next, click on ```[Data Sources]``` in the search management section of the side menu. This displays the list of data sources created for the account. Then, click on ```[+ Add Data Source]``` at the top menu, followed by clicking on ```[Add Data Source]```.

![Creating an AI Search data source](images/3.prepare-index/4.png)

A GUI-based screen for adding a data source appears. First, enter "sample-datasource" in the ```[Name]``` field. Then, specify the Azure Storage account you created earlier in the ```[Storage Account]``` field. Also, select "search-docs" for the ```[BLOB Container]``` field. Finally, check the option for ```[Authenticate using Managed ID]``` and click the ```[Create]``` button.

![Creating an AI Search data source](images/3.prepare-index/5.png)

### Creating the Skillset
Next, click on ```[Skillset]``` in the search management section of the side menu. This displays the list of skillsets created for the account. Then, click on ```[+ Add Skillset]``` in the top menu.

![Creating an AI Search skillset](images/3.prepare-index/6.png)

A screen for adding a skillset appears. Skillsets can only be defined using JSON (GUI configuration is not available). As with the index, you should essentially input the JSON definition from [skillset.json](../ai-search/skillset.json#L49), but replace the ```{OPENAI_SERVICE_NAME}``` at line 49 with the name of the AI Service account that received the AI Search role.

```json
{
    "@odata.type": "#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill",
    "context": "/document/markdown_document/*/pages/*",
    "resourceUri": "https://{OPENAI_SERVICE_NAME}.openai.azure.com",
    "deploymentId": "text-embedding-3-large",
    "modelName": "text-embedding-3-large",
    "dimensions": 3072,
    "inputs": [
        {
            "name": "text",
            "source": "/document/markdown_document/*/pages/*"
        }
    ],
    "outputs": [
        {
            "name": "embedding",
            "targetName": "text_vector"
        }
    ]
}
```

![Creating an AI Search skillset](images/3.prepare-index/7.png)

### Creating and Running the Indexer
Next, click on ```[Indexer]``` in the search management section of the side menu. This displays the list of indexers created for the account. Then, click on ```[+ Add Indexer]``` in the top menu, followed by clicking on ```[Add Indexer (JSON)]```.

![Creating and running the AI Search indexer](images/3.prepare-index/8.png)

Replace the originally entered JSON with the contents of [indexer.json](../ai-search/indexer.json) and click the ```[Save]``` button.

![Creating and running the AI Search indexer](images/3.prepare-index/9.png)

The indexer you just created appears in the indexer list and shows a status of `processing`. Click on the indexer named `sample-indexer` to view its details.

![Viewing details of the AI Search indexer](images/3.prepare-index/10.png)

In this screen, you can run the indexer, reset the record of which files in the data source have been processed (this prevents files from being processed twice), and check the indexer’s past execution history and success status. On the right side of the screen, you can see the execution result from when it was created—confirm that the `[Status]` is set to "success" and the `[Number of Successful Documents]` is "3".

![Verifying the execution of the AI Search indexer](images/3.prepare-index/11.png)

## Explanation of the Index JSON Definition

### Overview of the Definition: Root Fields
The overall structure of the index JSON definition is as follows:
```json
{
    "name": "sample-index",
    "fields": [],
    "semantic": {},
    "vectorSearch": {},
    "similarity": {}
}
```
In the `fields` array, you specify fields such as "id" and "text", and indicate which fields are to be searched. The `semantic` field defines settings related to semantic search, the `vectorSearch` field defines settings for vector search, and the `similarity` field defines settings for text search using BM25. There are also other root fields such as `corsOptions` for CORS settings, `scoringProfiles` for scoring profiles (a feature for weighting scores of fields in text search), and `suggesters` for query auto-completion. For more details, refer to the following documents:

> [Search index overview - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index)
> [Indexes - Create Or Update - REST API (Azure Search Service) | Microsoft Learn](https://learn.microsoft.com/en-us/rest/api/searchservice/indexes/create-or-update?view=rest-searchservice-2024-07-01&tabs=HTTP)

### Definition of Fields (fields)
The `fields` array defines the fields. Within it, fields are defined as shown below:

```json
{
    "fields": [
        {
            "name": "id",
            "type": "Edm.String",
            "key": true,
            "searchable": true,
            "filterable": false,
            "sortable": true,
            "facetable": false,
            "analyzer": "keyword"
        },
        {
            "name": "chunk",
            "type": "Edm.String",
            "searchable": true,
            "filterable": false,
            "sortable": false,
            "facetable": false,
            "analyzer": "en.microsoft"
        },
        {
            "name": "chunkVector",
            "type": "Collection(Edm.Single)",
            "searchable": true,
            "stored": false,
            "dimensions": 3072,
            "vectorSearchProfile": "vectorProfile"
        },
        ...
    ]
}
```

First, specify the field name with `name` and the data type with `type`. Supported data types include `Edm.String` (for strings), `Edm.Int32` (for 32-bit integers), and `Collection(Edm.Single)` (for 32-bit floating-point arrays, primarily used for storing vector values). For a full list of supported types, refer to [this document](https://learn.microsoft.com/en-us/rest/api/searchservice/supported-data-types). The field that represents the ID needs to have `key` set to true (the default is false).

For fields that are intended for text search, set `searchable` to true. This does not apply to fields used for vector search. For fields with `searchable` set to true, you can specify an analyzer using the `analyzer` property. An analyzer is used to split text into tokens (which are different from tokens in LLMs). Although English naturally tokenizes well using spaces, languages like Japanese require morphological analysis, making the choice of analyzer very important. You can select from Lucene analyzers, Microsoft’s language analyzers, or even custom analyzers composed of a tokenizer and filters. If no analyzer is specified, the default analyzer is used—but for Japanese, this default is not very effective—so be sure to set an appropriate language analyzer. For documents containing both Japanese and English, using the Japanese Lucene analyzer "ja.lucene" is recommended. For a list of supported analyzers, see [this document](https://learn.microsoft.com/en-us/azure/search/search-analyzers).

For fields intended for vector search, first specify the data type as either "Collection(Edm.Single)" or "Collection(Edm.Byte)" to store vector values, then define the number of dimensions using the `dimensions` property, and finally specify the name of the vector search configuration (profile) with `vectorSearchProfile`. The `stored` property determines whether the vector value is included in the searchable document; if set to false, the vector is used solely for search, which can significantly reduce storage size. The `filterable` property is important as it allows you to filter documents based on the field value (similar to a WHERE clause in SQL), and the `facetable` property determines whether the field can be used for faceted navigation—a feature that simplifies filtering options for users.

> [Supported data types - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/rest/api/searchservice/supported-data-types)
> [Analyzers for linguistic and text processing - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-analyzers)
> [Add a faceted navigation category hierarchy - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-faceted-navigation)

### Semantic Search Configuration (semantic)

Azure AI Search’s [Semantic Search](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview) applies L2 ranking techniques developed from Microsoft Bing’s search technology. In AI Search, this is known as semantic reranking—where the initial N search document results produced by text search, vector search, or hybrid search are re-scored using a small language model (SLM) to recalculate their relevance to the query. It’s essentially like taking a subset of documents from a hybrid search and re-searching them based on a language model. The following diagram illustrates the process flow from query submission, through hybrid search, Reciprocal Rank Fusion (RRF) for rank fusion, and finally semantic reranking:

![AI Search - Semantic Search](images/99.others/ai-search-semantic.png)

Semantic search settings are configured in the `semantic` field. You can define multiple semantic configurations under `configurations`, each with a `name` and a set of `prioritizedFields` used for reranking. The fields specified here correspond to those defined earlier. You can designate a `titleField` (for document titles), `prioritizedContentFields` (for main content), and `prioritizedKeywordsFields` (for keywords). These fields are used by the SLM to generate summaries and compute similarity with the query, integrating the results into the search score.

The `defaultConfiguration` field specifies which configuration is used when an application sends a search request to AI Search without specifying a particular configuration name.

```json
"semantic": {
    "defaultConfiguration": "semanticConfig",
    "configurations": [
        {
            "name": "semanticConfig",
            "prioritizedFields": {
                "titleField": {
                    "fieldName": "chapter"
                },
                "prioritizedContentFields": [
                    {
                        "fieldName": "chunk"
                    }
                ],
                "prioritizedKeywordsFields": [
                    {
                        "fieldName": "section"
                    },
                    {
                        "fieldName": "subsection"
                    }
                ]
            }
        }
    ]
}
```

> [Semantic ranking - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview)

### Vector Search Configuration (vectorSearch)

The `vectorSearch` field is used to configure settings related to vector search. Specifically, you define the search algorithms (`algorithms`), settings for vectorizing the query (`vectorizers`), and profiles (`profiles`) that combine an algorithm with a vectorizer. Additionally, you can configure vector quantization using the `compressions` field, which is useful if you need to reduce storage size when there are too many searchable documents in the index; for more details, see [this document](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-configure-compression-storage).

The goal is to define a profile referenced by the `vectorSearchProfile` property of your vector search field. To do this, you configure both a search algorithm and query vectorization settings.

In the `algorithms` field, you may specify multiple vector search algorithms. The supported algorithms for the `kind` property are kNN (exhaustiveKnn) and [HNSW](https://arxiv.org/abs/1603.09320) (hnsw). kNN performs a brute-force search and is suitable for small-scale queries (e.g., less than 10,000 documents), while HNSW is typically preferred. When using HNSW, you can specify algorithm parameters under `hnswParameters`. Generally speaking, increasing these values improves search accuracy (broadening the search scope) at the cost of slower performance. You can also specify the vector similarity metric using the `metric` property; if no specific requirement exists, setting it to `cosine` (cosine similarity) is sufficient.

The `vectorizers` field allows you to configure how the search query is vectorized. Multiple vectorizers can be defined, each explaining how to generate the vector representation for the query. In the example below, vectorization is performed using the text-embedding-3-large model from Azure OpenAI Service. However, you could also use models deployed via the [Azure AI Foundry Model Catalog](https://learn.microsoft.com/en-us/azure/search/vector-search-integrated-vectorization-ai-studio?tabs=inference-image), vectorization via a Web API, or even image vectorization. In this example, Microsoft Entra authentication is used for the OpenAI Service; alternatively, you could provide an API key via the `apiKey` property for key-based authentication, which is convenient for quick testing during development.

Finally, in the `profiles` field, you define profiles that combine the specified algorithms (`algorithms`) and vectorizers (`vectorizers`). Multiple profiles can be defined, each with its own `name`. The profile name is then referenced in a vector field via the `vectorSearchProfile` property, which tells the search engine to use the corresponding algorithm and query vectorization settings for that field.

```json
"vectorSearch": {
    "algorithms": [
        {
            "name": "algorithm",
            "kind": "hnsw",
            "hnswParameters": {
                "m": 10,
                "efConstruction": 1000,
                "efSearch": 1000,
                "metric": "cosine"
            }
        }
    ],
    "vectorizers": [
        {
            "name": "vectorizer",
            "kind": "azureOpenAI",
            "azureOpenAIParameters": {
                "resourceUri": "https://{OPENAI_SERVICE_NAME}.openai.azure.com",
                "deploymentId": "text-embedding-3-large",
                "modelName": "text-embedding-3-large"
            }
        }
    ],
    "profiles": [
        {
            "name": "vectorProfile",
            "algorithm": "algorithm",
            "vectorizer": "vectorizer"
        }
    ]
}
```

#### Appendix: HNSW (Hierarchical Navigable Small World)

![HNSW (Hierarchical Navigable Small World)](images/99.others/ai-search-hnsw.png)

#### Appendix: Vector Similarity Metrics Supported by Azure AI Search

![Vector similarity metrics supported by Azure AI Search](images/99.others/ai-search-similarity-metrics.png)

#### Appendix: Compression of Vector Fields

![Vector field compression in AI Search](images/99.others/ai-search-vector-compression.png)

> [Vector search - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/vector-search-overview)

### Text Search Configuration (similarity)

For text search, AI Search uses the [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) algorithm. In a nutshell, BM25 is similar to TF-IDF (Term Frequency-Inverse Document Frequency) in that it boosts search scores for documents where rare terms (across all documents) appear, while also favoring documents that contain many occurrences of the search terms—all while taking document length into account.

![BM25](images/99.others/ai-search-bm25.png)

In AI Search, you can configure the parameters `k1` (0.0–3.0) and `b` (0–1). The `k1` parameter controls the influence of term frequency across all documents (i.e. the rarity of the term), with higher values giving more weight to rare terms (a value of 0 neglects this factor entirely). The `b` parameter controls the influence of document length, with 0 meaning no consideration of document length. The default values of k1=1.2 and b=0.75 are usually acceptable, though using k1=3.0 might intensify keyword search, which could be beneficial in certain cases.

```json
"similarity": {
    "@odata.type": "#Microsoft.Azure.Search.BM25Similarity",
    "k1": 1.2,
    "b": 0.75
}
```

> [Configure BM25 relevance scoring - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/index-ranking-similarity)

## Explanation of the Data Source JSON Definition

In this guide we defined the data source using the GUI, but it is created using a JSON definition similar to the one below. Since the data source is Azure Blob Storage, the `type` is set to `azureblob`. For the types of data sources that can be specified in AI Search, refer to [this document](https://learn.microsoft.com/en-us/azure/search/search-data-sources-gallery). Additionally, information for accessing Blob Storage is provided via the `credentials` (for authentication information) and `container` (for container details). In this case, Microsoft Entra authentication is used by specifying a resource ID, but you could also use key authentication by providing the Blob Storage connection string (which includes the storage endpoint and authentication key).

```json
{
    "name": "sample-datasource",
    "type": "azureblob",
    "credentials": {
        "connectionString": "ResourceId=/subscriptions/{SUBSCRIPTION_ID}/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.Storage/storageAccounts/{STORAGE_ACCOUNT}/;"
    },
    "container": {
        "name": "search-docs"
    }
}
```

## Explanation of the Indexer JSON Definition

In this example, the indexer is defined as follows. The required definitions include the linked data source name (`dataSourceName`), skillset name (`skillsetName`), and index name (`targetIndexName`). You can also specify how much of the document’s data to provide to the skillset using settings within `parameters.configuration` (namely `dataToExtract` and `allowSkillsetToReadFileData`). This is also where you can filter the types of files to be processed by the indexer (for example, processing only PDF files).

Additionally, you can perform field mappings for the outputs (results and metadata) from the skillset. The `metadata_storage_name` in `fieldMappings` is metadata assigned when using Blob Storage as the data source—containing the blob name (`metadata_storage_name`) and the content type (`metadata_storage_content_type`). This value is stored in the index’s `documentName` field. For more details, see [this document](https://learn.microsoft.com/en-us/azure/search/search-howto-indexing-azure-blob-storage).

```json
{
    "name": "sample-indexer",
    "dataSourceName": "sample-datasource",
    "skillsetName": "sample-skillset",
    "targetIndexName": "sample-index",
    "parameters": {
        "configuration": {
            "parsingMode": "default",
            "dataToExtract": "contentAndMetadata",
            "allowSkillsetToReadFileData": true
        }
    },
    "fieldMappings": [
        {
            "sourceFieldName": "metadata_storage_name",
            "targetFieldName": "documentName"
        }
    ],
    "outputFieldMappings": []
}
```

## Explanation of the Skillset JSON Definition

### Overall Structure: Root Fields
The overall structure of the skillset JSON definition is as follows, with the primary definitions being `name` and `skills`. Additionally, if you are outputting multiple search documents (for example, splitting a single file such as a PDF into several documents), you must define [index projections](https://learn.microsoft.com/ja-jp/azure/search/search-how-to-define-index-projections?tabs=rest-create-index%2Crest-create-index-projection) under `indexProjections`.

```json
{
    "name": "sample-skillset",
    "skills": [],
    "indexProjections": {}
}
```

Other configurable settings include `cognitiveServices` for connecting to Azure AI Services, `knowledgeStore` for configuring the use of the [Knowledge Store](https://learn.microsoft.com/en-us/azure/search/knowledge-store-concept-intro?tabs=portal), and `encryptionKey` for encryption. For more details, see [this document](https://learn.microsoft.com/en-us/azure/search/cognitive-search-defining-skillset).

> [Skillset concepts - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets)
> [Create a skillset - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-defining-skillset)
> [Define index projections - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-how-to-define-index-projections?tabs=rest-create-index%2Crest-create-index-projection)

### Adding a Skill: Document Analysis

The `skills` property is an array where you add individual skills. First, to extract text from an input PDF file, add the [Document Layout Skill](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-document-intelligence-layout). It is recommended to refer to the official Microsoft documentation (Microsoft Learn) when adding skills—with the Document Layout Skill based on [this page](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-document-intelligence-layout).

First, verify the `@odata.type`. According to the official documentation, it should be set to [Microsoft.Skills.Util.DocumentIntelligenceLayoutSkill](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-document-intelligence-layout#section).

![Document Layout Skill type](images/99.others/ai-search-skill-type.png)

Next, specify the `context`. This is the root path for the input/output data processed by the skillset; for the first skillset, `/document` is sufficient. Then, specify the skill’s parameters. According to the official documentation, the Document Layout Skill accepts two parameters: `outputMode` and `markdownHeaderDepth`. Also, review the skill’s inputs (`inputs`) and outputs (`outputs`); for example, you can specify `file_data` as the input and receive `markdown_document` as the output.

![Skill Inputs](images/99.others/ai-search-skill-inputs.png)

![Skill Outputs](images/99.others/ai-search-skill-outputs.png)

You can also refer to the official documentation for sample definitions and outputs, and then add the following skill definition. The data input at `/document/file_data` (such as a PDF file) has a structure like this (shown in JSON for clarity):

```json
{
    "document": {
        "file_data": {
            "$type": "file",
            "contentType": "application/pdf",
            "data": "JVBERi0xLjQKJdPr6eEKMSAwIG9iago8PC9...",
            "url": "https://xxxxx.blob.core.windows.net/...."
        }
    }
}
```

For the input `text` (specified by the `source`), set it to `/document/file_data`. Also, output `markdown_document` as the skill output.

```json
{
    "@odata.type": "#Microsoft.Skills.Util.DocumentIntelligenceLayoutSkill",
    "context": "/document",
    "outputMode": "oneToMany",
    "markdownHeaderDepth": "h3",
    "inputs": [
        {
            "name": "file_data",
            "source": "/document/file_data"
        }
    ],
    "outputs": [
        {
            "name": "markdown_document",
            "targetName": "markdown_document"
        }
    ]
}
```

As a result, the output `markdown_document` will be generated as follows (shown in JSON for clarity):

```json
{
    "document": {
        "file_data": {},
        "markdown_document": [
            { 
                "content": "Interpolations set the shape of the slope...",
                "sections": { 
                    "h1": "Character filters add processing before a string reaches the tokenizer...",
                    "h2": "Character filters",
                    "h3": ""
                },
                "ordinal_position": 0
            }, 
            {},
            {},
            ...
        ]
    }
}
```

> [Document Layout skill - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-document-intelligence-layout)

### Adding a Skill: Chunk Splitting

Next, to split the text extracted by the Document Layout Skill into chunks, add the [Text Split Skill](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-textsplit). Although the Document Layout Skill may already divide text into chapters or sections, this additional skill handles very long chapters or sections. Refer to the [Skillset Reference](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-textsplit) to complete the JSON definition.

It is important to note that since `/document/markdown_document` is an array, you should append an asterisk (*) to process all elements within it; for example, use `/document/markdown_document/*` (see [reference](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-annotation-language)). Then, define the skill parameters, inputs, and outputs as before. In this case, text is split into chunks of 1024 tokens with an overlap of 256 tokens. For the skill input, since you want to use the `content` of each element of `/document/markdown_document`, specify `/document/markdown_document/*/content` to pass each array element as input.

```json
{
    "@odata.type": "#Microsoft.Skills.Text.SplitSkill",
    "context": "/document/markdown_document/*",
    "textSplitMode": "pages",
    "maximumPageLength": 1024,
    "pageOverlapLength": 256,
    "defaultLanguageCode": "en",
    "unit": "azureOpenAITokens",
    "azureOpenAITokenizerParameters": {
        "encoderModelName": "cl100k_base"
    },
    "inputs": [
        {
            "name": "text",
            "source": "/document/markdown_document/*/content"
        }
    ],
    "outputs": [
        {
            "name": "textItems",
            "targetName": "pages"
        }
    ]
}
```

The split chunks (an array of strings) are output as `textItems`. In this example they are output as `pages`, resulting in a structure like the following (shown in JSON for clarity). Since the context is `/document/markdown_document/*`, the output appears under `/document/markdown_document/*/pages`.

```json
{
    "document": {
        "file_data": {},
        "markdown_document": [
            { 
                "content": "Interpolations set the shape of the slope...",
                "sections": { 
                    "h1": "Character filters add processing before a string reaches the tokenizer...",
                    "h2": "Character filters",
                    "h3": ""
                },
                "pages": [
                    "Interpolations set the shape of the slope..."
                ],
                "ordinal_position": 0
            }, 
            {},
            {},
            ...
        ]
    }
}
```

> [Text split skill - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-textsplit)
> - [Skill context and input annotation reference language - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-annotation-language)

### Adding a Skill: Text Embedding

Finally, to generate embeddings (vector values) for the chunked text using Azure OpenAI Service, add the [Azure OpenAI Embedding Skill](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-azure-openai-embedding). As before, refer to the [reference](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-azure-openai-embedding) to complete the JSON definition. 

Set the `context` to target the array of strings in `pages` by specifying `/document/markdown_document/*/pages/*`. This ensures that every element in the `pages` array (within `markdown_document`) is processed. Then, as before, define the skill parameters, inputs, and outputs. This configuration primarily specifies the embedding model. In this example, Microsoft Entra authentication is used for Azure OpenAI Service, but you can also use API key authentication by specifying an API key in the `apiKey` property.

```json
{
    "@odata.type": "#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill",
    "context": "/document/markdown_document/*/pages/*",
    "resourceUri": "https://{OPENAI_SERVICE_NAME}.openai.azure.com",
    "deploymentId": "text-embedding-3-large",
    "modelName": "text-embedding-3-large",
    "dimensions": 3072,
    "inputs": [
        {
            "name": "text",
            "source": "/document/markdown_document/*/pages/*"
        }
    ],
    "outputs": [
        {
            "name": "embedding",
            "targetName": "text_vector"
        }
    ]
}
```

After executing this skill, the data structure will look like the following (illustrated in JSON for clarity). Since the `context` is `/document/markdown_document/*/pages/*`, the results will be output under `/document/markdown_document/*/pages/*/text_vector` (resulting in a slightly unconventional data structure).

```json
{
    "document": {
        "file_data": {},
        "markdown_document": [
            { 
                "content": "Interpolations set the shape of the slope...",
                "sections": { 
                    "h1": "Character filters add processing before a string reaches the tokenizer...",
                    "h2": "Character filters",
                    "h3": ""
                },
                "pages": [
                    {
                        "Interpolations set the shape of the slope...",
                        { "text_vector": [0.321780, 0.745397, ...] }
                    }
                ],
                "ordinal_position": 0
            }, 
            {},
            {},
            ...
        ]
    }
}
```

> [Azure OpenAI Embedding skill - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-azure-openai-embedding)

### Index Projection

When using an Azure Blob Storage data source in AI Search, by default a file results in a single search document unless [index projection](https://learn.microsoft.com/en-us/azure/search/search-how-to-define-index-projections?tabs=rest-create-index%2Crest-create-index-projection) is employed. In this case, since multiple search documents (the chunked text and its associated metadata) are generated from a single file, you define index projections to register them as separate search documents in the index.

Selectors are defined under the `selectors` property. A selector can be defined for each target index (allowing output to multiple indexes, if desired).

```json
"indexProjections": {
    "selectors": [
        {
            "targetIndexName": "sample-index",
            "parentKeyFieldName": "parentId",
            "sourceContext": "/document/markdown_document/*/pages/*",
            "mappings": [
                {
                    "name": "documentName",
                    "source": "/document/metadata_storage_name"
                },
                {
                    "name": "chunk",
                    "source": "/document/markdown_document/*/pages/*"
                },
                {
                    "name": "chunkVector",
                    "source": "/document/markdown_document/*/pages/*/text_vector"
                },
                {
                    "name": "chapter",
                    "source": "/document/markdown_document/*/sections/h1"
                },
                {
                    "name": "section",
                    "source": "/document/markdown_document/*/sections/h2"
                },
                {
                    "name": "subsection",
                    "source": "/document/markdown_document/*/sections/h3"
                }
            ]
        }
    ],
    "parameters": {
        "projectionMode": "skipIndexingParentDocuments"
    }
}
```

In the selector, you define the target index name (`targetIndexName`), the index field that will store the parent document ID (`parentKeyFieldName`), the root context (`sourceContext`) from which to split into multiple search documents, and the field mappings. In this index, `parentKeyFieldName` corresponds to [the following field](../ai-search/index.json#L15). When using index projection, the key field must be searchable (`searchable=true`) and use the keyword analyzer (`analyzer=keyword`).

```json
{
    "fields": [
        {
            "name": "id",
            "type": "Edm.String",
            "key": true,
            "searchable": true,
            "filterable": false,
            "sortable": true,
            "facetable": false,
            "analyzer": "keyword"
        },
        {
            "name": "parentId",
            "type": "Edm.String",
            "key": false,
            "searchable": false,
            "filterable": true,
            "sortable": false,
            "facetable": false
        },
        ...
    ]
}
```

In `mappings`, you define the relationship between the data produced by the skillset (variables under `/document`) and the index fields. In the example below, the index’s `chunkVector` field is populated with the generated embeddings of the chunks. The `/document/metadata_storage_name` is metadata assigned when using Blob Storage as the data source, which includes the blob’s name and content type. For more details, see [this document](https://learn.microsoft.com/en-us/azure/search/search-howto-indexing-azure-blob-storage).

```json
"mappings": [
    {
        "name": "documentName",
        "source": "/document/metadata_storage_name"
    },
    {
        "name": "chunk",
        "source": "/document/markdown_document/*/pages/*"
    },
    {
        "name": "chunkVector",
        "source": "/document/markdown_document/*/pages/*/text_vector"
    },
    ...
]
```

> [Define index projections - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-how-to-define-index-projections?tabs=rest-create-index%2Crest-create-index-projection)
> [Azure blob indexer - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/search-howto-indexing-azure-blob-storage)

## Verifying Operation with Debug Functionality

Using AI Search’s [Debug Sessions](https://learn.microsoft.com/en-us/azure/search/cognitive-search-debug-session), you can closely inspect the operation of your skillset and, if issues arise, iteratively modify and re-test the skillset configuration.

Click on ```[Debug Sessions]``` in the search management section of the AI Search account page; this displays a list of debug sessions created for the account. Then, click on ```[+ Add Debug Session]``` in the top menu. A window for creating a debug session appears on the right side—enter "sample-debug" as the debug session name, select ```[sample-indexer]``` for the ```[Indexer Template]```, check the option ```[Select First Document]```, and specify the storage account you created in the ```[Storage Account]``` field. Then click the ```[Save]``` button.

![Testing AI Search debug functionality](images/3.prepare-index/12.png)

The indexer execution will begin. Once the execution is complete, you will see a diagram and the generated data structure. The diagram on the left shows the relationships between the various skillsets and index mappings, and the window on the right displays the input/output values of each skillset.

![Using AI Search debug functionality](images/3.prepare-index/13.png)

> [Debug Sessions concepts - Azure AI Search | Microsoft Learn](https://learn.microsoft.com/en-us/azure/search/cognitive-search-debug-session)